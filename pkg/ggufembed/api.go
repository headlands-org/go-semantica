// Package ggufembed provides a high-level API for GGUF embedding models
package ggufembed

import (
	"context"
	"fmt"
	"runtime"
	"sync"

	modelrt "github.com/lth/pure-go-llamas/internal/runtime"
)

// Runtime is the main interface for the embedding runtime
type Runtime interface {
	// Embed generates embeddings for the given texts
	Embed(ctx context.Context, texts []string) ([][]float32, error)

	// EmbedSingle generates an embedding for a single text
	EmbedSingle(ctx context.Context, text string) ([]float32, error)

	// Close releases resources
	Close() error

	// EmbedDim returns the embedding dimension
	EmbedDim() int

	// MaxSeqLen returns the maximum sequence length
	MaxSeqLen() int
}

// embedRuntime implements Runtime
type embedRuntime struct {
	model   *modelrt.Model
	options Options
	pool    *sync.Pool // pool of worker resources
}

// Options configures the runtime
type Options struct {
	// NumThreads specifies the number of worker goroutines for parallel text processing.
	//
	// If set to 0 (default): Auto-tuned based on batch size using an adaptive strategy:
	//   - Batch 1-4:   1 worker (serial, avoids goroutine overhead)
	//   - Batch 5-16:  min(batch, NumCPU/4) workers (light parallelism)
	//   - Batch 17-32: min(batch, NumCPU/2) workers (moderate parallelism)
	//   - Batch 33+:   min(batch, NumCPU) workers (full CPU utilization)
	//
	// If set > 0: Uses the specified number of workers (no auto-tuning).
	//
	// Note: When DisableMatmulParallel=false (default), this controls coarse-grained
	// parallelism (across texts) while matmul uses fine-grained parallelism (within ops).
	// When DisableMatmulParallel=true, this is the only parallelism mechanism.
	//
	// Default: 0 (auto-tune)
	NumThreads int

	// BatchSize specifies the batch size for parallel processing (default: 1)
	// This controls how many texts are grouped together in a single goroutine.
	// Larger batch sizes reduce goroutine overhead but increase latency per batch.
	BatchSize int

	// Verbose enables verbose logging
	Verbose bool

	// DisableMatmulParallel disables internal matrix multiplication parallelism.
	// When true, matmul operations run serially, relying on coarse-grained parallelism
	// via NumThreads to process multiple texts in parallel. **Recommended for batch workloads.**
	// When false, uses fine-grained parallelism within matmul operations.
	//
	// **Performance Impact** (validated via profiling):
	//   - Batch >= 8 with DisableMatmulParallel=true: 34% faster, 9600x fewer goroutines
	//   - Single text: minimal difference (within 5%)
	//
	// Set to true when:
	//   - Processing batches of texts (batch >= 8) - **RECOMMENDED**
	//   - You want predictable single-text latency
	//   - You control parallelism at the application level
	//
	// Set to false when:
	//   - Processing single texts in isolation
	//   - You want maximum single-text throughput with nested parallelism
	//
	// Default: true (optimized for batch workloads)
	DisableMatmulParallel bool
}

// Option is a functional option for configuring the runtime
type Option func(*Options)

// WithThreads sets the number of threads
func WithThreads(n int) Option {
	return func(o *Options) {
		o.NumThreads = n
	}
}

// WithBatchSize sets the batch size
func WithBatchSize(n int) Option {
	return func(o *Options) {
		o.BatchSize = n
	}
}

// WithVerbose enables verbose logging
func WithVerbose(v bool) Option {
	return func(o *Options) {
		o.Verbose = v
	}
}

// WithDisableMatmulParallel disables internal matrix multiplication parallelism
func WithDisableMatmulParallel(disable bool) Option {
	return func(o *Options) {
		o.DisableMatmulParallel = disable
	}
}

// Open opens a GGUF model file and returns a Runtime
func Open(path string, opts ...Option) (Runtime, error) {
	options := Options{
		NumThreads:            0,     // 0 = auto-tune based on batch size
		BatchSize:             1,
		Verbose:               false,
		DisableMatmulParallel: true,  // Optimized for batch workloads (34% faster)
	}

	for _, opt := range opts {
		opt(&options)
	}

	// Load model
	model, err := modelrt.LoadModel(path, options.DisableMatmulParallel)
	if err != nil {
		return nil, fmt.Errorf("load model: %w", err)
	}

	rt := &embedRuntime{
		model:   model,
		options: options,
		pool: &sync.Pool{
			New: func() interface{} {
				return &workerState{}
			},
		},
	}

	return rt, nil
}

// workerState holds per-worker scratch space
type workerState struct {
	// Can add worker-specific buffers here if needed
}

// Embed generates embeddings for multiple texts
func (r *embedRuntime) Embed(ctx context.Context, texts []string) ([][]float32, error) {
	if len(texts) == 0 {
		return nil, nil
	}

	results := make([][]float32, len(texts))
	errors := make([]error, len(texts))

	// Auto-tune worker count based on batch size if not explicitly set
	// NOTE: Model is not thread-safe, so we use a semaphore to ensure only one
	// goroutine accesses the model at a time. The worker pool pattern here is
	// designed for future expansion when we have per-worker model instances.
	numWorkers := r.options.NumThreads
	if numWorkers <= 0 {
		numWorkers = r.autoTuneWorkers(len(texts))
	}

	// Use a semaphore to limit concurrent model access to 1
	// TODO: Create per-worker model instances to enable true parallelism
	sem := make(chan struct{}, 1) // Only 1 concurrent model access for now

	var wg sync.WaitGroup
	batchSize := r.options.BatchSize
	if batchSize <= 0 {
		batchSize = 1
	}

	for i := 0; i < len(texts); i += batchSize {
		end := i + batchSize
		if end > len(texts) {
			end = len(texts)
		}

		wg.Add(1)
		go func(start, end int) {
			defer wg.Done()

			for j := start; j < end; j++ {
				// Check context
				select {
				case <-ctx.Done():
					errors[j] = ctx.Err()
					return
				default:
				}

				// Acquire semaphore (ensure only 1 goroutine accesses model at a time)
				sem <- struct{}{}
				// Encode
				embedding, err := r.embedSingle(texts[j])
				<-sem // Release semaphore

				if err != nil {
					errors[j] = err
				} else {
					results[j] = embedding
				}
			}
		}(i, end)
	}

	wg.Wait()

	// Check for errors
	for _, err := range errors {
		if err != nil {
			return results, err
		}
	}

	return results, nil
}

// autoTuneWorkers determines the optimal number of worker threads based on batch size.
//
// Strategy (optimized for coarse-grained parallelism with DisableMatmulParallel=true):
//   - Batch 1-4:   Use 1 worker (serial processing avoids goroutine overhead)
//   - Batch 5-16:  Use min(batch, NumCPU/4) workers (light parallelism)
//   - Batch 17-32: Use min(batch, NumCPU/2) workers (moderate parallelism)
//   - Batch 33+:   Use min(batch, NumCPU) workers (full CPU utilization)
//
// Rationale:
//   - Small batches: Serial processing is more efficient due to low goroutine overhead
//   - Medium batches: Moderate parallelism balances throughput and latency
//   - Large batches: Full CPU utilization maximizes throughput
//   - Never exceed batch size: No benefit from idle workers
//
// This strategy is based on empirical benchmarks (see worker_tuning_test.go) which show:
//   1. Diminishing returns beyond NumCPU workers
//   2. Goroutine overhead dominates for small batches
//   3. Optimal scaling when workers â‰ˆ min(batch_size, NumCPU)
//
func (r *embedRuntime) autoTuneWorkers(totalTexts int) int {
	numCPU := runtime.NumCPU()

	// For single texts, always use 1 worker
	if totalTexts == 1 {
		return 1
	}

	// Small batches (1-4): Serial processing
	if totalTexts <= 4 {
		return 1
	}

	// Medium-small batches (5-16): Light parallelism
	if totalTexts <= 16 {
		workers := max(1, numCPU/4)
		return min(workers, totalTexts)
	}

	// Medium batches (17-32): Moderate parallelism
	if totalTexts <= 32 {
		workers := max(1, numCPU/2)
		return min(workers, totalTexts)
	}

	// Large batches (33+): Full CPU utilization
	return min(numCPU, totalTexts)
}

// max returns the maximum of two integers
func max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

// min returns the minimum of two integers
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

// EmbedSingle generates an embedding for a single text
func (r *embedRuntime) EmbedSingle(ctx context.Context, text string) ([]float32, error) {
	// Check context
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}

	return r.embedSingle(text)
}

// embedSingle is the internal implementation
func (r *embedRuntime) embedSingle(text string) ([]float32, error) {
	// Tokenize
	tokenIDs, err := r.model.Tokenizer().Encode(text)
	if err != nil {
		return nil, fmt.Errorf("tokenize: %w", err)
	}

	// Check length
	if len(tokenIDs) > r.model.Config().MaxSeqLen {
		return nil, fmt.Errorf("sequence too long: %d > %d", len(tokenIDs), r.model.Config().MaxSeqLen)
	}

	// Forward pass
	embedding, err := r.model.Forward(tokenIDs)
	if err != nil {
		return nil, fmt.Errorf("forward: %w", err)
	}

	return embedding, nil
}

// Close releases resources
func (r *embedRuntime) Close() error {
	return r.model.Close()
}

// EmbedDim returns the embedding dimension
func (r *embedRuntime) EmbedDim() int {
	return r.model.Config().EmbedDim
}

// MaxSeqLen returns the maximum sequence length
func (r *embedRuntime) MaxSeqLen() int {
	return r.model.Config().MaxSeqLen
}
