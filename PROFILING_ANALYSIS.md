# Profiling Analysis & Top 5 Optimization Opportunities

**Date**: 2025-10-25
**Profile**: CPU profile of BenchmarkForwardINT8 (20 iterations, 27.4ms/op average)
**Platform**: AMD Ryzen 9 7900 (x86-64, AVX512 VNNI enabled)

---

## Executive Summary

Current performance: **25-27ms per inference** ("Hello world", 4 tokens)

**Hottest path**: Matrix multiplication dominates at **68% of total CPU time**
- INT8 quantized matmul: 68.18% (1.95s / 2.86s)
- SIMD dot product kernel: 30.42% (0.87s / 2.86s)

**Quick wins available**: RoPE frequency pre-computation, GELU approximation, memory pooling
**Estimated speedup from top 5 optimizations**: **1.5-2.0x** (25ms â†’ 12-17ms)

---

## Profiling Data

### Top Functions by Cumulative Time

| Function | Flat % | Cum % | Time | Description |
|----------|--------|-------|------|-------------|
| `matMulQ8_0INT8Parallel.func1` | 36.36% | **68.18%** | 1.95s | Parallel matmul worker goroutine |
| `dotProductINT8SIMD` | 13.64% | **30.42%** | 0.87s | INT8 SIMD dot product dispatcher |
| `dotProductINT8VNNI` | 16.78% | 16.78% | 0.48s | AVX512 VNNI assembly kernel |
| `runAttentionINT8` | 0% | **7.69%** | 0.22s | Attention mechanism |
| `ApplyRoPE` | 0% | **5.24%** | 0.15s | Rotary position embeddings |
| `runMLPINT8` | 0% | **2.80%** | 0.08s | MLP feed-forward |
| `extractQ8_0Scales` | 1.40% | 3.50% | 0.10s | Scale extraction (model load) |

### Hot Paths Breakdown

**Matrix Multiplication (68% of time)**:
```
matMulQ8_0INT8Parallel.func1 (1.95s)
â”œâ”€ Line 451: dotProductINT8SIMD (910ms) - SIMD dot product
â”œâ”€ Line 453: FP32 accumulation (570ms) - float32(sum) * scale * input.Scale
â”œâ”€ Line 450: unsafe.Slice() (90ms) - Zero-copy byteâ†’int8
â””â”€ Overhead (380ms) - Loop control, index calculation
```

**Attention Mechanism (7.69% of time)**:
```
runAttentionINT8 (220ms)
â”œâ”€ ApplyRoPE (150ms)
â”‚  â”œâ”€ math.Pow (100ms) - Frequency calculation in loop!
â”‚  â””â”€ math.Cos/Sin (50ms) - Trigonometric functions
â”œâ”€ MatMulQ8_0INT8 (30ms) - Q/K/V projections
â””â”€ MultiHeadAttention (10ms) - Attention scores + softmax
```

**MLP Feed-Forward (2.80% of time)**:
```
runMLPINT8 (80ms)
â”œâ”€ GELU activation (40ms) - Expensive exp/tanh
â”œâ”€ QuantizeSymmetricINT8 (20ms) - INT8 quantization
â”œâ”€ MatMulQ8_0INT8 (10ms) - Gate/Up projections
â””â”€ VecMulF32 (10ms) - Element-wise multiply
```

---

## Top 5 Optimization Opportunities

### 1. ðŸ”¥ Optimize INT8 Matmul Kernel (Expected: 1.3-1.5x speedup)

**Current bottleneck**: 68% of CPU time in matrix multiplication

**Issue**: Per-block overhead in parallel worker
```go
// Current: called millions of times per inference
for blockIdx := 0; blockIdx < blocksPerRow; blockIdx++ {
    scaleIdx := j*blocksPerRow + blockIdx              // Index calculation
    scale := scales[scaleIdx]                          // Memory access
    qsOffset := blockOffset + 2                        // Pointer arithmetic
    weightSlice := unsafe.Slice(...)                   // Slice construction
    blockSum := dotProductINT8SIMD(inputSlice, ...)    // Function call
    sum += float32(blockSum) * scale * input.Scale     // FP32 accumulation
}
```

**Optimization strategies**:

#### A. **Fused Block Processing** (1.2x speedup)
Process multiple blocks (4-8) in a single SIMD operation:
```go
// Process 4 blocks at once (128 int8s)
blockSum4 := dotProductINT8VNNI_4Blocks(&a[0], &b[0], &scales[0])
// Returns: [sum0, sum1, sum2, sum3] in a single vector
```

Benefits:
- Reduce function call overhead by 4x
- Better instruction pipelining
- Amortize scale loading across multiple blocks

#### B. **Blocked Matrix Layout** (1.1x speedup)
Reorder weight data for better cache locality:
```go
// Current: row-major (cache-unfriendly for repeated access)
weights: [row0_block0, row0_block1, ..., row1_block0, row1_block1, ...]

// Optimized: blocked layout (cache-friendly)
weights: [row0-3_block0, row0-3_block1, ..., row4-7_block0, ...]
```

Benefits:
- 4x better cache utilization
- Prefetch works better
- Enables vectorized outer loop

#### C. **Inline Critical Path** (1.05x speedup)
```go
// Use go:inline directive
//go:inline
func dotProductINT8VNNI(a, b *int8, n int) int32

// Or expand manually in hot loop for small blocks
if blockSize == 32 {
    // Inline AVX512 VNNI for common case
    blockSum = inlinedVNNI32(inputPtr, weightPtr)
}
```

**Expected impact**: 1.3-1.5x total speedup from all three (25ms â†’ 17-19ms)

---

### 2. ðŸŽ¯ Pre-compute RoPE Frequencies (Expected: 1.1x speedup)

**Current bottleneck**: 5.24% of CPU time (150ms), **math.Pow dominates at 100ms**

**Issue**: Frequency calculation in innermost loop
```go
// Called per-token, per-head, per-dimension!
// For 4 tokens Ã— 3 heads Ã— 128 dims = 1,536 calls
for i := 0; i < halfDim; i++ {
    freq := float32(1.0 / math.Pow(float64(base), float64(2*i)/float64(headDim)))  // 100ms!
    theta := p * freq
    cos := float32(math.Cos(float64(theta)))  // 50ms
    sin := float32(math.Sin(float64(theta)))
}
```

**Optimization**:
```go
// Pre-compute at model load time
type RoPECache struct {
    freqs []float32  // Pre-computed frequencies [headDim/2]
    cos   []float32  // Cached cos values [maxSeqLen][headDim/2]
    sin   []float32  // Cached sin values [maxSeqLen][headDim/2]
}

func (m *Model) buildRoPECache() {
    halfDim := m.config.HeadDim / 2
    m.ropeCache.freqs = make([]float32, halfDim)

    // Compute frequencies once
    for i := 0; i < halfDim; i++ {
        m.ropeCache.freqs[i] = float32(1.0 / math.Pow(
            float64(m.config.RoPEBase),
            float64(2*i)/float64(m.config.HeadDim)))
    }

    // Pre-compute cos/sin for common positions
    maxCachePos := 2048
    m.ropeCache.cos = make([]float32, maxCachePos*halfDim)
    m.ropeCache.sin = make([]float32, maxCachePos*halfDim)

    for pos := 0; pos < maxCachePos; pos++ {
        for i := 0; i < halfDim; i++ {
            theta := float32(pos) * m.ropeCache.freqs[i]
            m.ropeCache.cos[pos*halfDim+i] = float32(math.Cos(float64(theta)))
            m.ropeCache.sin[pos*halfDim+i] = float32(math.Sin(float64(theta)))
        }
    }
}

// Usage in ApplyRoPE
func (m *Model) ApplyRoPE(qk []float32, seqLen, nHeads, headDim int, pos []int) {
    halfDim := headDim / 2
    for s := 0; s < seqLen; s++ {
        p := pos[s]
        cosPtr := m.ropeCache.cos[p*halfDim:]
        sinPtr := m.ropeCache.sin[p*halfDim:]

        for h := 0; h < nHeads; h++ {
            offset := (s*nHeads + h) * headDim
            for i := 0; i < halfDim; i++ {
                cos := cosPtr[i]  // Cache lookup!
                sin := sinPtr[i]
                // Rotation...
            }
        }
    }
}
```

**Memory cost**: ~2MB for 2048 positions Ã— 128 dims Ã— 8 bytes (cos+sin)
**Expected impact**: Eliminate 150ms â†’ ~5ms (1.08x total speedup: 25ms â†’ 23ms)

---

### 3. âš¡ Fast GELU Approximation (Expected: 1.02x speedup)

**Current bottleneck**: 40ms in MLP (1.4% of total time)

**Issue**: Exact GELU uses expensive transcendental functions
```go
// Current: exact GELU
func GELU(dst, src []float32, n int) {
    for i := 0; i < n; i++ {
        x := src[i]
        dst[i] = 0.5 * x * (1 + float32(math.Tanh(
            float64(math.Sqrt(2.0/math.Pi) * (x + 0.044715*x*x*x)))))
    }
}
```

**Optimization**: Use fast polynomial approximation
```go
// Fast GELU approximation (99.5% accurate, 5x faster)
func GELUFast(dst, src []float32, n int) {
    const c1 = 0.5
    const c2 = 0.7978845608 // sqrt(2/Ï€)
    const c3 = 0.044715

    for i := 0; i < n; i++ {
        x := src[i]
        x3 := x * x * x
        tanh_arg := c2 * (x + c3*x3)

        // Fast tanh approximation: tanh(x) â‰ˆ x / (1 + |x|) for small x
        // For better accuracy: use rational approximation
        tanh_val := tanhApprox(tanh_arg)
        dst[i] = c1 * x * (1 + tanh_val)
    }
}

// Or use SwiGLU which is faster and equally effective
func SwiGLU(gate, up []float32, n int) {
    for i := 0; i < n; i++ {
        // Swish(gate) * up
        swish := gate[i] / (1 + float32(math.Exp(-float64(gate[i]))))
        gate[i] = swish * up[i]
    }
}
```

**Better option**: Vectorize with SIMD
```go
//go:noescape
func geluAVX2(dst, src *float32, n int)

// Assembly: process 8 float32s per iteration
// Use AVX2 polynomial approximation
// 8-10x faster than scalar version
```

**Expected impact**: 40ms â†’ 5-8ms (1.015x total speedup: 25ms â†’ 24.6ms)

---

### 4. ðŸŽ± Memory Pool for Temporary Buffers (Expected: 1.05x speedup)

**Current bottleneck**: Repeated allocations in hot path

**Issue**: Every layer allocates temporary buffers
```go
// In runAttentionINT8 (called 24 times per inference)
q := make([]float32, seqLen*embDim)          // 4Ã—768 = 3KB
k := make([]float32, seqLen*kvDim)           // 4Ã—256 = 1KB
v := make([]float32, seqLen*kvDim)           // 1KB
attnOut := make([]float32, seqLen*embDim)    // 3KB
attnScratch := make([]float32, seqLen*seqLen*nHeads)  // 4Ã—4Ã—3 = 48 floats

// In runMLPINT8 (called 24 times per inference)
gate := make([]float32, seqLen*intermDim)    // 4Ã—2304 = 9KB
up := make([]float32, seqLen*intermDim)      // 9KB

// Total per layer: ~26KB Ã— 24 layers = 624KB allocated per inference
```

**Optimization**: Pre-allocate buffer pool
```go
type BufferPool struct {
    qkv       []float32  // seqLen*maxDim*3 (reused for Q, K, V)
    attnOut   []float32  // seqLen*embDim
    attnWork  []float32  // seqLen*seqLen*nHeads
    mlpGate   []float32  // seqLen*intermDim*2 (gate + up)
}

func (m *Model) initBufferPool(maxSeqLen int) {
    maxDim := max(m.config.EmbedDim, m.config.IntermDim)
    m.bufPool.qkv = make([]float32, maxSeqLen*maxDim*3)
    m.bufPool.attnOut = make([]float32, maxSeqLen*m.config.EmbedDim)
    m.bufPool.attnWork = make([]float32, maxSeqLen*maxSeqLen*m.config.NumHeads)
    m.bufPool.mlpGate = make([]float32, maxSeqLen*m.config.IntermDim*2)
}

// Usage
func (m *Model) runAttentionINT8(...) {
    embDim := m.config.EmbedDim
    kvDim := m.config.NumKVHeads * m.config.HeadDim

    // Slice the pre-allocated pool
    q := m.bufPool.qkv[:seqLen*embDim]
    k := m.bufPool.qkv[seqLen*embDim : seqLen*embDim+seqLen*kvDim]
    v := m.bufPool.qkv[seqLen*embDim+seqLen*kvDim : seqLen*embDim+seqLen*kvDim*2]
    attnOut := m.bufPool.attnOut[:seqLen*embDim]
}
```

**Benefits**:
- Eliminate 624KB allocations per inference
- Reduce GC pressure (6.64% of CPU time in GC)
- Better cache locality (warm memory)

**Expected impact**: 1.05x speedup (25ms â†’ 23.8ms), reduces GC time

---

### 5. ðŸ§® Batch Quantization Operations (Expected: 1.03x speedup)

**Current bottleneck**: 20ms per inference in QuantizeSymmetricINT8

**Issue**: Quantization has two-pass overhead
```go
// Current: Two passes over data
func QuantizeSymmetricINT8(data []float32, rows, cols int) QuantizedTensorINT8 {
    // Pass 1: Find max absolute value
    absMax := float32(0)
    for i := range data {
        if abs := math.Abs(float64(data[i])); float32(abs) > absMax {
            absMax = float32(abs)
        }
    }

    // Pass 2: Quantize
    scale := absMax / 127.0
    quantized := make([]int8, len(data))
    for i := range data {
        quantized[i] = int8(math.Round(float64(data[i] / scale)))
    }
}
```

**Optimization 1**: Fuse passes with SIMD
```go
// Single-pass quantization with SIMD
//go:noescape
func quantizeINT8AVX2(dst *int8, src *float32, n int) (scale float32)

// Assembly pseudo-code:
// 1. SIMD max absolute value (AVX2 horizontal max)
// 2. Broadcast scale = absMax/127
// 3. SIMD multiply + round + convert to int8
// 4. Store results
// All in one pass with excellent vectorization
```

**Optimization 2**: Per-row quantization (better accuracy)
```go
// Current: single scale for entire tensor
scale := absMax / 127.0  // One scale for all rows

// Better: per-row scales (already done for weights)
scales := make([]float32, rows)
for row := 0; row < rows; row++ {
    rowData := data[row*cols : (row+1)*cols]
    scales[row] = computeRowScale(rowData)
    quantizeRow(quantized[row*cols:], rowData, scales[row])
}
```

Benefits:
- Better quantization accuracy (fewer outliers dominate)
- Parallelizable across rows
- Matches Q8_0 weight format (per-block scales)

**Expected impact**: 20ms â†’ 12-15ms (1.025x total speedup: 25ms â†’ 24.4ms)

---

## Cumulative Impact Analysis

### Speedup Calculation

| Optimization | Current (ms) | After (ms) | Speedup | Cumulative (ms) |
|--------------|--------------|------------|---------|-----------------|
| **Baseline** | 25.00 | - | 1.00x | 25.00 |
| 1. INT8 matmul fusion | 17.00 | 11.30 | 1.50x | 19.30 |
| 2. RoPE pre-computation | 3.75 | 0.12 | 31.25x | 15.67 |
| 3. Fast GELU | 1.00 | 0.20 | 5.00x | 15.47 |
| 4. Memory pooling | (GC) | - | 1.05x | 14.73 |
| 5. Batch quantization | 0.50 | 0.30 | 1.67x | 14.53 |

**Total estimated speedup**: **1.72x** (25ms â†’ 14.5ms)

### Conservative Estimate

Accounting for Amdahl's Law and measurement error:
- **Best case**: 1.7x speedup (25ms â†’ 14.7ms)
- **Realistic**: 1.5x speedup (25ms â†’ 16.7ms)
- **Worst case**: 1.3x speedup (25ms â†’ 19.2ms)

**Target**: Match or beat llama.cpp (16ms) with optimization #1-2 alone

---

## Implementation Priority

### Phase 1: Quick Wins (1-2 days)
1. **RoPE pre-computation** - Easiest, clear 3.75ms savings
2. **Fast GELU approximation** - Simple polynomial, 1ms savings
3. **Memory pooling** - Reduces GC pressure, improves consistency

**Expected**: 1.15x speedup (25ms â†’ 21.7ms)

### Phase 2: Kernel Optimization (3-5 days)
4. **Fused block processing** - Batch 4 blocks in SIMD
5. **Inline critical paths** - Reduce function call overhead

**Expected**: Additional 1.25x speedup (21.7ms â†’ 17.4ms)

### Phase 3: Advanced (1 week)
6. **Blocked matrix layout** - Reorder weights for cache locality
7. **Per-row quantization** - Better accuracy and parallelism
8. **Full assembly matmul** - Custom AVX512 kernel

**Expected**: Additional 1.15x speedup (17.4ms â†’ 15.1ms)

**Final target**: **15ms** (1.67x total speedup, matches llama.cpp performance)

---

## Recommendations

### Immediate Actions (This Sprint)
1. âœ… **Implement RoPE caching** - Low risk, high reward (3.75ms savings)
2. âœ… **Add fast GELU** - Simple change, measurable impact (1ms savings)
3. âœ… **Create buffer pool** - Reduces GC jitter, improves tail latency

### Next Sprint
4. **Prototype fused INT8 matmul** - Process 4 blocks at once
5. **Benchmark blocked layouts** - Validate cache improvements
6. **Profile again** - Measure actual gains, adjust priorities

### Long-term (Future Work)
- **Custom AVX512 matmul** - Full control over instruction sequence
- **INT4 quantization** - 2x faster memory bandwidth (accuracy trade-off)
- **KV cache optimization** - For multi-token generation
- **Flash Attention** - O(N) attention for long sequences

---

## Appendix: Profiling Commands

```bash
# Generate CPU profile
go test -tags=integration -bench=BenchmarkForwardINT8 -benchtime=20x \
    -cpuprofile=cpu.prof -run=^$ ./internal/runtime

# Analyze top functions
go tool pprof -top cpu.prof

# Detailed function view
go tool pprof -list="matMulQ8_0INT8Parallel" cpu.prof

# Cumulative time breakdown
go tool pprof -cum -top cpu.prof

# Interactive analysis
go tool pprof -http=:8080 cpu.prof
```

---

**Generated**: 2025-10-25
**Next Review**: After Phase 1 implementation
