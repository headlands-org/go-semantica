=== Single Long Document (49w) Performance Analysis ===

AFTER OPTIMIZATION (inputScale hoisting):
Total Time: 39.37s for 50 embeddings = 787ms per embedding
Improvement: 15.7% faster (148ms saved per embedding)

BASELINE (before optimization):
Total Time: 46.73s for 50 embeddings = 935ms per embedding

TOP BOTTLENECKS (AFTER OPTIMIZATION):

1. Matrix Multiplication (80% of time - 32.75s, down from 37.43s)
   - matMulQ8_0INT8Serial: 22.15s flat, 32.75s cumulative

   BREAKDOWN:
   - Line 327 (accumulation): 12s (down from 17.13s - 29% improvement!)
     sum += float32(blockSum) * scale

   - Line 324 (SIMD dot product): 10.57s (virtually unchanged)
     blockSum := dotProductINT8Asm(inputPtr, weightPtr, 32)

   - Line 347 (final scale): 2.21s (new, expected cost)
     dst[i*outDim+j] = sum * inputScale

   - Line 317 (scale lookup): 3.61s
     scale := scales[scaleBaseIdx+blockIdx]

   - Line 316 (offset calc): 2.40s
     blockOffset := weightRowOffset + blockIdx*34

   REMAINING ISSUE: Accumulation still taking 1.13× longer than SIMD computation
   This suggests:
   - Memory bandwidth bottleneck (scale lookups)
   - Need SIMD vectorization for accumulation
   - Loop unrolling could help
   
   CODE STRUCTURE:
   for i := 0; i < batch; i++ {           // sequence positions
     for j := 0; j < outDim; j++ {        // output dimensions (768+)
       for blockIdx := 0; blockIdx < fullBlocks; blockIdx++ {  // many blocks
         blockSum := dotProductINT8Asm(...)  // 10.49s - GOOD (SIMD)
         sum += float32(blockSum) * scale * inputScale  // 17.13s - BAD!
       }
     }
   }

2. Attention Computation (9% of time - 4.40s)
   - MultiHeadAttentionWithScale: 4.29s flat
   
   BREAKDOWN:
   - Lines 88-89 (Q·K dot product): 2.01s
     for d := 0; d < headDim; d++ {
       score += Q[qOffset+d] * K[kOffset+d]
     }
   
   - Lines 121-122 (weighted sum): 2.13s
     for d := 0; d < headDim; d++ {
       output[outOffset+d] += weight * V[vOffset+d]
     }
   
   ISSUE: Scalar loops, no SIMD vectorization

3. GC Overhead (4% of time - 1.98s)
   - gcDrain, scanobject, greyobject
   - Some allocation happening despite buffer pools

OPTIMIZATION RESULTS:

✅ COMPLETED - Hoist inputScale multiplication (DONE):
   - Before: 17.13s on accumulation line
   - After: 12s accumulation + 2.21s final scale = 14.21s
   - Saved: 2.92s (17% improvement on accumulation)
   - Overall: 935ms → 787ms per embedding (15.7% faster)

NEXT OPTIMIZATION OPPORTUNITIES (Ranked by Impact):

HIGH IMPACT:
1. Further optimize matmul accumulation (12s → target 5-6s)
   - Loop unrolling (process 2-4 blocks at once)
   - Reduce scale lookup overhead (3.61s on line 317)
   - Vectorize accumulation with FMA instructions

2. Vectorize attention dot products (3.95s → target 1-2s)
   - Replace scalar loops with SIMD
   - Use dotProductINT8 pattern for Q·K and weighted sum

MEDIUM IMPACT:
3. Reduce GC pressure (1.99s → target 0.5s)
   - Pre-allocate scratch buffers
   - Pool more aggressively

EXPECTED IMPROVEMENT:
Current: 787ms per embedding (15.7% improvement achieved)
Next target with loop unrolling: ~600-650ms per embedding
Ultimate target with all optimizations: ~400-500ms per embedding

VALIDATION:
For comparison, batch mode achieves ~76ms per 49-word embedding
when running 96 in parallel. Single-threaded should be able to
achieve ~400-500ms with all optimizations (batch has parallelism overhead).
