═══════════════════════════════════════════════════════════════════════════════
                    ROOFLINE PERFORMANCE ANALYSIS
          AMD Ryzen 9 7900 (Zen 4) - EmbeddingGemma 300M Q8_0
═══════════════════════════════════════════════════════════════════════════════

                        THEORETICAL PEAK PERFORMANCE
                        ─────────────────────────────

    Single-Core INT8:  576 GOPS  (at 4.5 GHz all-core boost)
    12-Core INT8:      6,912 GOPS
    Memory Bandwidth:  83.2 GB/s  (DDR5-5200 dual-channel)

                        ┌─────────────────────────────┐
                        │   Theoretical Ceiling       │
                        │      576 GOPS (1-core)      │ ← 100%
                        └─────────────────────────────┘
                                     ▲
                                     │ 2.7× headroom
                                     │
                        ┌─────────────────────────────┐
                        │   llama.cpp Achieved        │
                        │      212 GOPS (36.8%)       │ ← llama.cpp
                        └─────────────────────────────┘
                                     ▲
                                     │ 8.5× gap
                                     │
                        ┌─────────────────────────────┐
                        │   pure-go-llamas Current    │
                        │      24.9 GOPS (4.3%)       │ ← We are here
                        └─────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════
                        COMPUTE vs MEMORY BOUND
═══════════════════════════════════════════════════════════════════════════════

    Roofline Threshold: 83 ops/byte
    ─────────────────────────────────────────────────────────────────────────

    Arithmetic Intensity:
      • Cold run (model load):  4.2 ops/byte   → MEMORY-BOUND ⚠
      • Warm run (in cache):    254 ops/byte   → COMPUTE-BOUND ✓

    ┌─────────────────────────────────────────────────────────────────┐
    │                                                                 │
    │  6,912 ┤                           ┌──────────────────────────  │
    │  GOPS  │                           │  COMPUTE-BOUND             │
    │        │                           │  (Our warm-run region)     │
    │  3,456 ┤                           │                            │
    │        │                           │                            │
    │        │                           │                            │
    │    576 ┤                           │  llama.cpp ●               │
    │        │                    ╱      │                            │
    │    212 ┤                 ╱         │                            │
    │        │              ╱  Roofline  │                            │
    │     25 ┤  ●──────────              │  pure-go-llamas ●          │
    │        │  │                        │                            │
    │      0 └──┼────────────────────────┼────────────────────────────┤
    │           4.2        83           254                           │
    │         (cold)   (threshold)    (warm)    Arithmetic Intensity  │
    │                                              (ops/byte)          │
    └─────────────────────────────────────────────────────────────────┘

    CONCLUSION: After warmup, we are COMPUTE-BOUND (not memory-bound).
                The bottleneck is execution efficiency, not bandwidth.


═══════════════════════════════════════════════════════════════════════════════
                        LATENCY COMPARISON (9-word doc)
═══════════════════════════════════════════════════════════════════════════════

    Theoretical Best:   2.2 ms  ┃ (100% efficiency - impossible)
                                ┃
                                ┃ 2.7× headroom from llama.cpp
                                ┃
    llama.cpp:          6.0 ms  ┃███████████▌ (36.8% efficiency)
                                ┃
                                ┃ 8.5× gap (our opportunity)
                                ┃
    pure-go-llamas:    51.1 ms  ┃█████████████████████████████████████████████████
                                ┃████████████████████████████████████████████████
                                ┃█████████████████████████████ (4.3% efficiency)


═══════════════════════════════════════════════════════════════════════════════
                        OPTIMIZATION ROADMAP
═══════════════════════════════════════════════════════════════════════════════

    Current State:      51.1 ms  (4.3% efficiency)
                          │
                          │  PHASE 1: SIMD Optimization
                          │  • Profile hot kernels (matmul, dequant)
                          │  • Optimize AVX2 assembly
                          │  • Improve cache locality
                          ▼
    Near-Term Goal:     13 ms    (16% efficiency)  → 4× improvement
                          │
                          │  PHASE 2: Scalar Overhead Reduction
                          │  • Minimize bounds checks
                          │  • Optimize hot loops
                          │  • Use Go PGO (Profile-Guided Optimization)
                          ▼
    Stretch Goal:       6 ms     (37% efficiency)  → Match llama.cpp
                          │
                          │  PHASE 3: Diminishing Returns
                          │  • May hit Go compiler limits
                          │  • Further gains become marginal
                          ▼
    Theoretical Max:    2.2 ms   (100% efficiency) → Unrealistic


═══════════════════════════════════════════════════════════════════════════════
                        KEY TAKEAWAYS
═══════════════════════════════════════════════════════════════════════════════

    ✓  Theoretical Peak:       576 GOPS (single-core INT8 @ 4.5 GHz)
    ✓  Memory Bandwidth:       83.2 GB/s (DDR5-5200)
    ✓  Workload Type:          COMPUTE-BOUND (after warmup)
    ✓  llama.cpp Efficiency:   36.8% (excellent for real-world inference)
    ✓  Headroom (llama→peak):  2.7× (difficult to extract)
    ✓  Our Efficiency Gap:     8.6× behind llama.cpp
    ✓  Optimization Potential: 8.5× speedup to match llama.cpp

    Focus Areas:
      1. SIMD kernel optimization (AVX2 matmul, dequant)
      2. Cache locality improvements (blocking, prefetch)
      3. Scalar overhead reduction (hot loops, bounds checks)

    Do NOT Focus On:
      ✗  Parallelism (already optimal for batches)
      ✗  Memory bandwidth (not the bottleneck)
      ✗  Algorithmic changes (correctness validated)

═══════════════════════════════════════════════════════════════════════════════
